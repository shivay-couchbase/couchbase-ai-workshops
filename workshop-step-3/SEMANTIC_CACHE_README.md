# Semantic Cache Implementation for RAG

This implementation adds semantic caching to the RAG application using Couchbase's vector search capabilities, similar to the nushell implementation but using pure JavaScript SDK.

## Overview

Semantic caching stores LLM responses based on semantic similarity of queries, not exact string matching. This means that similar queries (even with different wording) can retrieve cached responses, significantly improving performance and reducing costs.

## Architecture

```
User Query ‚Üí Semantic Cache Check ‚Üí Cache Hit/Miss
    ‚Üì
Cache Miss ‚Üí RAG Pipeline ‚Üí LLM Response ‚Üí Store in Cache
    ‚Üì
Cache Hit ‚Üí Return Cached Response
```

## Key Components

### 1. Semantic Cache Service (`services/semanticCacheService.js`)

- **`cachePut(prompt, llmString, response, ttlMinutes)`**: Stores cache entries with embeddings
- **`cacheGet(prompt, llmString, similarityThreshold, k)`**: Retrieves cached responses using vector similarity
- **`semanticCache(prompt, llmString, missHandler, options)`**: High-level API for cache operations
- **`createLLMSignature(model, temperature, maxTokens, systemPrompt)`**: Creates consistent LLM signatures

### 2. Updated Query Route (`routes/query.js`)

- Integrated semantic caching into the RAG pipeline
- Uses cache-first approach with fallback to fresh generation
- Maintains streaming response format

### 3. Setup Scripts

- **`scripts/setup-cache.js`**: Creates cache bucket and collection
- **`scripts/test-cache.js`**: Tests semantic cache functionality

## Setup Instructions

### 1. Create Cache Infrastructure

```bash
# Run the setup script to create cache bucket and collection
npm run setup-cache
```

### 2. Create Search Index

After running the setup script, you'll need to create a search index in Couchbase Capella:

1. Go to your Couchbase Capella dashboard
2. Navigate to **Search & Indexing**
3. Create a new index with:
   - **Index Name**: `cache._default.semantic_cache_idx`
   - **Source**: `cache._default.semantic`
   - **Index Definition**: Use the JSON provided by the setup script

### 3. Test the Implementation

```bash
# Test the semantic cache functionality
npm run test-cache
```

## Configuration

### Environment Variables

Add these to your `.env` file:

```env
# Existing variables...
COUCHBASE_CONNECTION_STRING=your-connection-string
COUCHBASE_USERNAME=your-username
COUCHBASE_PASSWORD=your-password
COUCHBASE_BUCKET_NAME=your-bucket-name
COUCHBASE_SEARCH_INDEX_NAME=your-index-name

# Cache configuration (optional - defaults provided)
CACHE_BUCKET=semantic_cache
CACHE_SCOPE=_default
CACHE_COLLECTION=semantic
```

### Cache Options

```javascript
const options = {
  similarityThreshold: 0.85,  // Minimum similarity score (0-1)
  k: 3,                       // Number of candidates to retrieve
  ttlMinutes: 1440           // TTL in minutes (24 hours default)
}
```

## How It Works

### 1. Cache Storage

When a cache miss occurs:
1. Query is processed through the full RAG pipeline
2. Response is generated by the LLM
3. Query embedding is created using OpenAI
4. Document is stored in cache with:
   - Original prompt
   - LLM signature (model, temperature, etc.)
   - Generated response
   - Vector embedding
   - TTL for automatic expiration

### 2. Cache Retrieval

When a query comes in:
1. Query embedding is created
2. Vector similarity search is performed
3. Results are filtered by LLM signature
4. Similarity threshold is applied
5. Best match is returned if above threshold

### 3. LLM Signature

The LLM signature ensures that responses are only cached for the same model configuration:
- Model name (e.g., "gpt-4")
- Temperature setting
- Max tokens
- System prompt

## Benefits

1. **Performance**: Cached responses are returned instantly
2. **Cost Reduction**: Reduces API calls to LLM providers
3. **Consistency**: Similar queries get consistent responses
4. **Scalability**: Handles high query volumes efficiently

## Monitoring

The implementation includes logging to track cache performance:

```
üéØ Cache HIT - returning cached response
‚ùå Cache MISS - calling miss handler
üìä Cache FRESH: uuid-1234-5678
```

## Advanced Features

### Custom Similarity Thresholds

Adjust the similarity threshold based on your use case:
- **0.9+**: Very strict matching (exact semantic meaning)
- **0.8-0.9**: Good balance (recommended)
- **0.7-0.8**: More lenient matching
- **<0.7**: Very lenient (may return irrelevant results)

### TTL Management

Cache entries automatically expire based on TTL:
- **Short TTL (1-6 hours)**: For frequently changing content
- **Medium TTL (12-24 hours)**: For stable content
- **Long TTL (7+ days)**: For static documentation

### Cache Clearing

```javascript
import { clearCache } from './services/semanticCacheService.js'

// Clear all cache entries
await clearCache()

// Clear entries for specific LLM signature
await clearCache('gpt-4:temp=0.7:max=1000:system=...')
```

## Troubleshooting

### Common Issues

1. **Index Not Found**: Ensure the search index is created with the correct name
2. **Bucket Not Found**: Run the setup script to create the cache bucket
3. **Low Cache Hit Rate**: Adjust similarity threshold or check LLM signature consistency
4. **Memory Issues**: Reduce TTL or implement cache size limits

### Debug Mode

Enable detailed logging by setting:
```javascript
process.env.DEBUG_CACHE = 'true'
```

## Performance Considerations

1. **Embedding Generation**: Each query requires an embedding call to OpenAI
2. **Vector Search**: KNN search performance depends on index size
3. **Memory Usage**: Cache grows over time - implement cleanup strategies
4. **Network Latency**: Consider cache location relative to your application